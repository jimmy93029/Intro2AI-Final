{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmy93029/Intro2AI-Final/blob/main/AI_Final_BASALT_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxwl_yG1qhR7"
      },
      "source": [
        "<div style=\"text-align: center\">\n",
        "  <img src=\"https://github.com/KarolisRam/MineRL2021-Intro-baselines/blob/main/img/colab_banner.png?raw=true\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_BGlQwngccr"
      },
      "source": [
        "# Introduction\n",
        "This notebook is the installation part for the [MineRL 2022](https://minerl.io/) competition, building on the original introductory notebooks created for the MineRL 2021 competition.\n",
        "\n",
        "## Note: About this file\n",
        "\n",
        "This file is updated by NYCU 2024 Spring Intro2AI Team 11: まふまふ.\n",
        "The original file is come from [here](https://colab.research.google.com/drive/1rJ3lGy-bG7kJRe_wYBWg7fjSaD9oOMDw?usp=sharing)\n",
        "\n",
        "## There's a video to explain...\n",
        "Please visit [this intro YouTube video](https://youtu.be/8yIrWcyWGek) to see some background information.  Hopefully, this will lead to a number of additional videos that explore what can be done in this environment...\n",
        "\n",
        "And if you see me=@mdda online, then please say \"Hi!\"\n",
        "\n",
        "## Software 2.0\n",
        "The approach we are going to use, where we took some human written code and replaced it with an AI component is quite similar to how Tesla approaches self driving cars. See this talk by Andrej Karpathy, Director of AI at Tesla:  \n",
        "[Building the Software 2.0 Stack](https://databricks.com/session/keynote-from-tesla)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysSTXmT3YUeF"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_HTScYNljgXv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb xserver-xephyr vnc4server python-opengl ffmpeg\n",
        "# Takes ~1min to run this\n",
        "# New Add\n",
        "!sudo apt-get install -y xvfb  # Install Xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xh6gb3UWjT3p"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# This takes ~22mins - which would hit us every time we start Colab\n",
        "#   So we'll do it once, and store a '.tar.gz' of the installation into our\n",
        "#   Google Drive, so that we can get it back much quicker the second time!\n",
        "\n",
        "##%%capture\n",
        "##!pip3 install --upgrade minerl # Default is 0.4.4, we want 1.0.0 for VPT\n",
        "##!pip3 uninstall minerl\n",
        "#!pip3 install git+https://github.com/minerllabs/minerl@v1.0.0\n",
        "#\n",
        "#!pip3 install pyvirtualdisplay\n",
        "#!pip3 install -U colabgymrender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sV7hXcp1vgj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c50ffd0-7105-4cfd-946c-c3cdfb45c1fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('mine_env', '/content/mine_env', 'mine_env.tar.gz')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os, sys, time\n",
        "\n",
        "mine_env = 'mine_env'\n",
        "mine_env_full = f'/content/{mine_env}'\n",
        "mine_tar = f'{mine_env}.tar.gz'\n",
        "\n",
        "if mine_env_full not in sys.path:\n",
        "  sys.path.insert(0, mine_env_full)\n",
        "  os.environ['PYTHONPATH'] += f':{mine_env_full}'\n",
        "\n",
        "mine_env, mine_env_full, mine_tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uvUh7yQ36RjQ",
        "outputId": "30a811cc-e056-4911-ecc7-5b825d5a4c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "-r-------- 1 root root 1504139165 Jun  1 04:41 ./mine_env.tar.gz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DONE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# We'll connect to our Google Drive here, and see whether we've already saved off a copy\n",
        "#   This will ask permission to 'connect to your drive' : The answer is 'Yes'!\n",
        "MINE_ENV_IS_NEW = True\n",
        "\n",
        "from google.colab import drive  # google.colab contains functions specifically for interacting with Google Colab's environment.\n",
        "drive.mount('/content/drive')    # mounts your Google Drive as a local file system\n",
        "if os.path.isfile(f'/content/drive/MyDrive/pythonLib/{mine_tar}'): # check if \"mine_env.tar.gz\" is in your Google Drive\n",
        "  ! cp /content/drive/MyDrive/pythonLib/$mine_tar ./$mine_tar  # ! means the command is to be executed in the shell rather than as Python code.\n",
        "                                              # This command copies the file from your Google Drive to the current working directory of the Colab notebook.\n",
        "\n",
        "  ! ls -l ./$mine_tar                         # This lists the file details such as permissions, owner, size, and modification date for the copied file in the current directory.\n",
        "                                              # It helps verify that the file has been copied correctly and shows its properties.\n",
        "  # e.g.: -rw------- 1 root root 1510118446 Jun 26 08:48 ./mine_env.tar.gz\n",
        "\n",
        "  # ! tar -tzf ./$mine_tar | grep minerl | head -5    # list some contents of the compressed tar file without extracting it\n",
        "  ! tar -xzf ./$mine_tar    # This extracts the contents of the tar file into the current directory\n",
        "\n",
        "  MINE_ENV_IS_NEW = False\n",
        "  # Takes 1min too (huge saving!)\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/pythonLib')\n",
        "sys.path.append('/content/drive/MyDrive/pythonLib/VPT')\n",
        "\n",
        "\"DONE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n7ZYcRvnvq6y",
        "outputId": "d34cc1d9-ddc5-4d89-991f-1fc2b4f67620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DONE, with MINE_ENV_IS_NEW=False'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Build the mine_env if necessary\n",
        "try:\n",
        "  from pyvirtualdisplay import Display\n",
        "except :\n",
        "  !pip3 install --target=$mine_env git+https://github.com/minerllabs/minerl@v1.0.2   # 21 mins\n",
        "  # https://stackoverflow.com/questions/55833509/attributeerror-type-object-callable-has-no-attribute-abc-registry\n",
        "  !mv $mine_env/typing.py $mine_env/MEH-typing.py  # Fix for Python3.7 ...\n",
        "\n",
        "  !pip3 install --target=$mine_env pyvirtualdisplay  # 4 secs  #注 Display creates a virtual framebuffer that graphical applications can use to render output as if they were using a real monitor.\n",
        "                                                              #注 This allows you to run applications that require a GUI without having an actual GUI environment installed on the system.\n",
        "  !pip3 install --target=$mine_env --upgrade colabgymrender # 22 secs  #注 colabgymrender provides a workaround by capturing the graphical output of the environment and displaying it within the notebook.\n",
        "\n",
        "  MINE_ENV_IS_NEW = True\n",
        "  # NB: some restart notices in the output ... but there's no need to restart!\n",
        "  #     In any case, please wait for the 'DONE' message to print out\n",
        "f\"DONE, with MINE_ENV_IS_NEW={MINE_ENV_IS_NEW}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNj2G3933yVu"
      },
      "outputs": [],
      "source": [
        "# check content of mine_env (execute if needed)\n",
        "! du -b mine_env | tail -5  # mine_env = ~ 2,094,031,775 bytes overall (a little bit less)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KOxO92nU4C8Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "08040962-20c8-465a-a301-fd36371f99f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DONE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Build new env.tar.gz file in google drive (execute if needed)\n",
        "if MINE_ENV_IS_NEW: #  or True\n",
        "  # ! ls -l /gdrive/MyDrive/mine*\n",
        "  ! rm -f ./$mine_tar   #注 removes the existing tar.gz archive of the environment, if any, from the current directory.\n",
        "  ! tar -czf ./$mine_tar $mine_env  #注 This command creates a new compressed (gzipped) tar archive of the directory specified by the $mine_env variable (the environment directory).\n",
        "  ! ls -l ./$mine_tar\n",
        "  # Without running the env...\n",
        "  # -rw-r--r-- 1 root root 1505020174 Jun 26 07:26 ./mine_env.tar.gz\n",
        "  # Once the minerl env has been reset once (i.e. java has built...)\n",
        "  # -rw------- 1 root root 1511976116 Jun 26 08:43 ./mine_env.tar.gz\n",
        "  ! tar -tzf ./$mine_tar | head\n",
        "  ! cp ./$mine_tar /content/drive/MyDrive/pythonLib/  #注 This copies the newly created archive to a Google Drive directory.\n",
        "  ! ls -l /content/drive/MyDrive/pythonLib/$mine_tar\n",
        "\"DONE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmrUKxvYXGa"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g8_vZpMFpiD9"
      },
      "outputs": [],
      "source": [
        "import os   # For interacting with the operating system.\n",
        "import time\n",
        "\n",
        "import numpy as np  # For numerical operations.\n",
        "\n",
        "import gym    # To create and manage environments based on the OpenAI Gym toolkit.\n",
        "import minerl\n",
        "\n",
        "from tqdm.notebook import tqdm  # For displaying progress bars in Jupyter notebooks.\n",
        "from colabgymrender.recorder import Recorder # To facilitate rendering of Gym environments in Google Colab.\n",
        "from pyvirtualdisplay import Display # To create a virtual display to render environments in a headless server or environment like Google Colab.\n",
        "\n",
        "import logging\n",
        "logging.disable(logging.ERROR) # reduce clutter, remove if something doesn't work to see the error logs.\n",
        "\n",
        "np.__version__  # '1.21.6' => that this is reading from our ~/mine_env directory\n",
        "# Numpy version may be different from the content above\n",
        "# About warning: since warning is in a local package, so if error occurs, please comment the specific line\n",
        "\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "#from PIL import Image\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import torch as th\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from run_inverse_dynamics_model import json_action_to_env_action\n",
        "\n",
        "import torch\n",
        "from torch.nn import Module, Sequential, Linear, Tanh, Parameter, Embedding\n",
        "from torch.distributions import Categorical, MultivariateNormal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QwEA7yJ6Ail"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0u4GNgalm8B",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from download_dataset import download_file\n",
        "download_file(100) # default is 400, about 40 GB?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjkM-WKjCSGx"
      },
      "source": [
        "# Construct Inverse Dynamic Model Agent\n",
        "\n",
        "Optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ehehsdr8CaG-"
      },
      "outputs": [],
      "source": [
        "from inverse_dynamics_model import load_IDM_agent\n",
        "IDMAgent = load_IDM_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oh2aHrFCHrMm"
      },
      "outputs": [],
      "source": [
        "# Test for IDMAgent\n",
        "# from agent import ENV_KWARGS # need to modify\n",
        "# required_resolution = ENV_KWARGS[\"resolution\"]\n",
        "# files = glob.glob(\"/content/MineRLBasaltFindCave-v0/*.mp4\")\n",
        "# video_path = files[0]\n",
        "# json_path = video_path.replace(\".mp4\", \".jsonl\")\n",
        "\n",
        "# cap = cv2.VideoCapture(video_path)\n",
        "# frames = []\n",
        "\n",
        "# json_index = 0\n",
        "# with open(json_path) as json_file:\n",
        "#   json_lines = json_file.readlines()\n",
        "#   json_data = \"[\" + \",\".join(json_lines) + \"]\"\n",
        "#   json_data = json.loads(json_data)\n",
        "\n",
        "# for _ in range(5000):\n",
        "#   ret, frame = cap.read()\n",
        "#   break\n",
        "#   if not ret:\n",
        "#     break\n",
        "#   assert frame.shape[0] == required_resolution[1] and frame.shape[1] == required_resolution[0], \"Video must be of resolution {}\".format(required_resolution)\n",
        "#   # BGR -> RGB\n",
        "#   frames.append(frame[..., ::-1])\n",
        "#   break\n",
        "#   if len(frames) == 100 or len(frames) == 50:\n",
        "#     l = len(frames)\n",
        "#     fs = np.stack(frames)\n",
        "#     predicted_actions = IDMAgent.predict_actions(fs)\n",
        "#     for i in range(50):\n",
        "#       env_action, _ = json_action_to_env_action(json_data[json_index])\n",
        "#       json_index += 1\n",
        "#       for y, (action_name, action_array) in enumerate(predicted_actions.items()):\n",
        "#         print(f\"{action_name}: {action_array[0, (l - 50 + i)]} ({env_action[action_name]}), \", end = \"\")\n",
        "#       print(\"\\n\")\n",
        "#     frames = frames[50:99]\n",
        "\n",
        "# predicted_actions = IDMAgent.predict_actions(fs)\n",
        "# l = len(frames)\n",
        "# for i in range(50, l):\n",
        "#   env_action, _ = json_action_to_env_action(json_data[json_index])\n",
        "#   json_index += 1\n",
        "#   for y, (action_name, action_array) in enumerate(predicted_actions.items()):\n",
        "#     print(f\"{action_name}: {action_array[0, (l - 50 + i)]} ({env_action[action_name]}), \", end = \"\")\n",
        "#   print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Nwtwork for vision Transformer"
      ],
      "metadata": {
        "id": "oLbtef0sWjd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform of env action and agent action\n",
        "env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
        "\n",
        "NOOP = env.action_space.no_op()\n",
        "\n",
        "# binary encoding of env_action\n",
        "# forward, back, left, right, sneak, sprint(run), jump, ESC = 2^7, ..., 2^0\n",
        "ACTION_LIST = [\"forward\", \"back\", \"left\", \"right\", \"sneak\", \"sprint\", \"jump\", \"ESC\"]\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "else:\n",
        "    from torch import FloatTensor\n",
        "\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB1zhE3OVlCr",
        "outputId": "e802eebb-baa8-4395-d2ab-391e1a3f4dc0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "uRXazKO3xbSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml-collections"
      ],
      "metadata": {
        "id": "18Hsz5rQ0nXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "from collections import OrderedDict  # pylint: disable=g-importing-member\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import ml_collections\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR"
      ],
      "metadata": {
        "id": "UbvJkKTOxRIB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## configs"
      ],
      "metadata": {
        "id": "pu5v6KqtymTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_testing():\n",
        "    \"\"\"Returns a minimal configuration for testing.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 1\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 1\n",
        "    config.transformer.num_heads = 1\n",
        "    config.transformer.num_layers = 1\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_b16_config():\n",
        "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 768\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 3072\n",
        "    config.transformer.num_heads = 12\n",
        "    config.transformer.num_layers = 12\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_r50_b16_config():\n",
        "    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n",
        "    config = get_b16_config()\n",
        "    del config.patches.size\n",
        "    config.patches.grid = (14, 14)\n",
        "    config.resnet = ml_collections.ConfigDict()\n",
        "    config.resnet.num_layers = (3, 4, 9)\n",
        "    config.resnet.width_factor = 1\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_b32_config():\n",
        "    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n",
        "    config = get_b16_config()\n",
        "    config.patches.size = (32, 32)\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_l16_config():\n",
        "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 1024\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 4096\n",
        "    config.transformer.num_heads = 16\n",
        "    config.transformer.num_layers = 24\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_l32_config():\n",
        "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
        "    config = get_l16_config()\n",
        "    config.patches.size = (32, 32)\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_h14_config():\n",
        "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n",
        "    config.hidden_size = 1280\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 5120\n",
        "    config.transformer.num_heads = 16\n",
        "    config.transformer.num_layers = 32\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "CONFIGS = {\n",
        "    'ViT-B_16': get_b16_config(),\n",
        "    'ViT-B_32': get_b32_config(),\n",
        "    'ViT-L_16': get_l16_config(),\n",
        "    'ViT-L_32': get_l32_config(),\n",
        "    'ViT-H_14': get_h14_config(),\n",
        "    'R50-ViT-B_16': get_r50_b16_config(),\n",
        "    'testing': get_testing(),\n",
        "}\n"
      ],
      "metadata": {
        "id": "_utN9qm8xcyh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network ( image detection )"
      ],
      "metadata": {
        "id": "Q6WKwO0qWUZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "class StdConv2d(nn.Conv2d):\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight\n",
        "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
        "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
        "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
        "                        self.dilation, self.groups)\n",
        "\n",
        "\n",
        "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
        "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=bias, groups=groups)\n",
        "\n",
        "\n",
        "def conv1x1(cin, cout, stride=1, bias=False):\n",
        "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
        "                     padding=0, bias=bias)"
      ],
      "metadata": {
        "id": "w301b0Z9w3eJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreActBottleneck(nn.Module):\n",
        "    \"\"\"Pre-activation (v2) bottleneck block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
        "        super().__init__()\n",
        "        cout = cout or cin\n",
        "        cmid = cmid or cout//4\n",
        "\n",
        "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
        "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
        "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
        "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n",
        "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
        "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if (stride != 1 or cin != cout):\n",
        "            # Projection also with pre-activation according to paper.\n",
        "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
        "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Residual branch\n",
        "        residual = x\n",
        "        if hasattr(self, 'downsample'):\n",
        "            residual = self.downsample(x)\n",
        "            residual = self.gn_proj(residual)\n",
        "\n",
        "        # Unit's branch\n",
        "        y = self.relu(self.gn1(self.conv1(x)))\n",
        "        y = self.relu(self.gn2(self.conv2(y)))\n",
        "        y = self.gn3(self.conv3(y))\n",
        "\n",
        "        y = self.relu(residual + y)\n",
        "        return y\n",
        "\n",
        "    def load_from(self, weights, n_block, n_unit):\n",
        "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
        "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
        "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
        "\n",
        "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
        "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
        "\n",
        "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
        "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
        "\n",
        "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
        "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
        "\n",
        "        self.conv1.weight.copy_(conv1_weight)\n",
        "        self.conv2.weight.copy_(conv2_weight)\n",
        "        self.conv3.weight.copy_(conv3_weight)\n",
        "\n",
        "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
        "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
        "\n",
        "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
        "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
        "\n",
        "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
        "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
        "\n",
        "        if hasattr(self, 'downsample'):\n",
        "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
        "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
        "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
        "\n",
        "            self.downsample.weight.copy_(proj_conv_weight)\n",
        "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
        "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))"
      ],
      "metadata": {
        "id": "fnTr35G7qUPI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetV2(nn.Module):\n",
        "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
        "\n",
        "    def __init__(self, block_units, width_factor):\n",
        "        super().__init__()\n",
        "        width = int(64 * width_factor)\n",
        "        self.width = width\n",
        "\n",
        "        # The following will be unreadable if we split lines.\n",
        "        # pylint: disable=line-too-long\n",
        "        self.root = nn.Sequential(OrderedDict([\n",
        "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
        "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
        "            ('relu', nn.ReLU(inplace=True)),\n",
        "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
        "        ]))\n",
        "\n",
        "        self.body = nn.Sequential(OrderedDict([\n",
        "            ('block1', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
        "                ))),\n",
        "            ('block2', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
        "                ))),\n",
        "            ('block3', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
        "                ))),\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.root(x)\n",
        "        x = self.body(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PREPWkhtqW1v"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vision transformer (network)"
      ],
      "metadata": {
        "id": "n-i6-kYzwz1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
      ],
      "metadata": {
        "id": "apCvos0uwr1-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "W_7YDEHbrBRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights\n"
      ],
      "metadata": {
        "id": "HHBT7Rafqbim"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "vp6fwGBgrD32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3d0yG0tIqgiS"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "Btx59aRUrFS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.hybrid = None\n",
        "        img_size = _pair(img_size)\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "            self.hybrid = False\n",
        "\n",
        "        if self.hybrid:\n",
        "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
        "                                         width_factor=config.resnet.width_factor)\n",
        "            in_channels = self.hybrid_model.width * 16\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=patch_size,\n",
        "                                       stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.hybrid:\n",
        "            x = self.hybrid_model(x)\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "T2-VP_GxqkrG"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## block"
      ],
      "metadata": {
        "id": "CQDXUpdirIMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config, vis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))"
      ],
      "metadata": {
        "id": "wWI4qFSiqodj"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "xfM8BfHfrLWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights"
      ],
      "metadata": {
        "id": "FbJD81gZqrDa"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "iRxL-oyqrOmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size)\n",
        "        self.encoder = Encoder(config, vis)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights"
      ],
      "metadata": {
        "id": "nysQjRxMqx-r"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "\n",
        "        self.transformer = Transformer(config, img_size, vis)\n",
        "        self.head = Linear(config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits, attn_weights\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.weight)\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)"
      ],
      "metadata": {
        "id": "6FREN_FsqyhY"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALExNet"
      ],
      "metadata": {
        "id": "__g7e3VbAzI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "\n",
        "  def __init__(self, output_size = 256):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 96, kernel_size = 11, stride = 4)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size = 5, padding = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size = 3, padding = 1)\n",
        "    self.conv4 = nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = 3, padding = 1)\n",
        "    self.conv5 = nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = 3, padding = 1)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features = 256*6*6, out_features = 4096)\n",
        "    self.fc2 = nn.Linear(in_features = 4096, out_features = 4096)\n",
        "    self.fc3 = nn.Linear(in_features = 4096, out_features = output_size)\n",
        "\n",
        "  def forward(self, input): # 227 * 227 * 3 RGB tensor\n",
        "    x = F.relu(self.conv1(input))\n",
        "    x = F.max_pool2d(x, kernel_size = 3, stride = 2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, kernel_size = 3, stride = 2)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = F.relu(self.conv5(x))\n",
        "    x = F.max_pool2d(x, kernel_size = 3, stride = 2)\n",
        "    x = th.flatten(x, start_dim = 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "    return x # int between [0, 255]"
      ],
      "metadata": {
        "id": "11RWnhjoA11E"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "TanYaKSTscU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "\n",
        "def save_model(model, output_dir, name):\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    model_checkpoint = os.path.join(output_dir, \"%s_checkpoint.bin\" % name)\n",
        "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
        "    logger.info(\"Saved model checkpoint to [DIR: %s]\", output_dir)\n",
        "\n",
        "def setup(num_classes, pretrained_dir, model_type, img_size, mlp_dim, dropout_rate):\n",
        "    # Prepare model\n",
        "    config = CONFIGS[model_type]\n",
        "\n",
        "    num_classes = 10\n",
        "\n",
        "    model = VisionTransformer(config, img_size=224, num_classes=21843, zero_head=False, vis=False)\n",
        "    model.load_from(np.load(pretrained_dir))\n",
        "    model.to(device)\n",
        "    num_params = count_parameters(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return params/1000000\n",
        "\n",
        "\n",
        "def set_seed(seed, n_gpu):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "class WarmupLinearSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then linear decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Linearly decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1, self.warmup_steps))\n",
        "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))"
      ],
      "metadata": {
        "id": "p4tVp5yqz2sN"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def env_action_to_agent(env_action: dict):\n",
        "    target_action_C = int(0)\n",
        "    target_action_R = env_action[\"camera\"]\n",
        "    for act in ACTION_LIST:\n",
        "        target_action_C *= 2\n",
        "        target_action_C += 1 if env_action.get(act) == 1 else 0\n",
        "    if target_action_C == 0 and np.array_equal(target_action_R, np.zeros(2)):\n",
        "        isNoop = True\n",
        "    else:\n",
        "        isNoop = False\n",
        "    return [target_action_C, target_action_R, isNoop]\n",
        "\n",
        "def agent_action_to_env(agent_action_C, agent_action_R):\n",
        "    target_action = NOOP\n",
        "    ACTION_LIST_Rev = ACTION_LIST.copy()\n",
        "    ACTION_LIST_Rev.reverse()\n",
        "    for act in ACTION_LIST_Rev:\n",
        "        target_action[act] = 1 if agent_action_C % 2 == 1 else 0\n",
        "        agent_action_C //= 2  # Use integer division to keep agent_action_C as an integer\n",
        "    target_action[\"camera\"] = agent_action_R\n",
        "    return target_action\n",
        "\n",
        "def img_to_tensor(frames):\n",
        "  target_tensor = th.empty((0, 3, 227, 227), dtype = th.float32)\n",
        "  for frame in frames:\n",
        "    frame = cv2.resize(frame, (227, 227))\n",
        "    frame = TF.to_tensor(frame).unsqueeze(0)\n",
        "    target_tensor = th.cat((target_tensor, frame), dim = 0)\n",
        "  return target_tensor"
      ],
      "metadata": {
        "id": "DAYGRY6Qy6bm"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FindCave Agent"
      ],
      "metadata": {
        "id": "_j77xXWrK_33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyperparameter"
      ],
      "metadata": {
        "id": "IJcRRkggwYDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1\n",
        "loggername = None  # The logger name is not specified in the argparse\n",
        "learning_rate = 3e-2\n",
        "\n",
        "seed = 42\n",
        "n_gpu = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
        "local_rank = -1\n",
        "\n",
        "n_epochs = 10\n",
        "warmup_steps = 100\n",
        "train_batch_size = 512\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "pretrained_dir = 'pretrained/ViT-B_16.npz'\n",
        "model_type = 'ViT-B_16'\n",
        "model = 'ViT-B_16'\n",
        "output_dir = \"output\"\n"
      ],
      "metadata": {
        "id": "370ETBnVwNPo"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://storage.googleapis.com/vit_models/imagenet21k/{model}.npz"
      ],
      "metadata": {
        "id": "YBPq6ubNt0iw"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "kwqZB-y3zkpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FindCaveAgent:\n",
        "    def __init__(self):\n",
        "        self.VIT = setup(num_classes=256, pretrained_dir=pretrained_dir, model_type=model_type, img_size=227, mlp_dim = 5, dropout_rate = 0.1)\n",
        "        self.optimizerVIT = optim.SGD(self.VIT.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "        self.schedulerVIT = WarmupLinearSchedule(self.optimizerVIT, warmup_steps=warmup_steps, t_total=t_total)\n",
        "\n",
        "        self.policyR = AlexNet(output_size=2).to(device)\n",
        "        self.optimizerR = optim.Adam(self.policyR.parameters(), lr=learning_rate)\n",
        "        self.lossRFunc = nn.MSELoss()\n",
        "\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.logger = logging.getLogger(loggername)\n",
        "        set_seed(seed, n_gpu)\n",
        "        self.n_gpu = n_gpu\n",
        "        self.local_rank = local_rank\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "    def train(self):\n",
        "        self.VIT.train()\n",
        "        self.policyR.train()\n",
        "\n",
        "        video_paths = glob.glob(\"/content/MineRLBasaltFindCave-v0/*.mp4\")\n",
        "        json_paths = [vp.replace(\".mp4\", \".jsonl\") for vp in video_paths]\n",
        "\n",
        "        batch_count = 0\n",
        "        batch_loss_C = 0\n",
        "        batch_loss_R = 0\n",
        "\n",
        "        losses = AverageMeter()\n",
        "        global_step = 0\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            print(f\"Epoch {epoch + 1}\")\n",
        "            cap_list = []\n",
        "            json_data_list = []\n",
        "            cur_json_index = []\n",
        "            for video_path, json_path in zip(video_paths, json_paths):\n",
        "                cap = cv2.VideoCapture(video_path)\n",
        "                cap_list.append(cap)\n",
        "                with open(json_path) as jf:\n",
        "                    json_lines = jf.readlines()\n",
        "                    json_data = \"[\" + \",\".join(json_lines) + \"]\"\n",
        "                    json_data = json.loads(json_data)\n",
        "                    json_data_list.append(json_data)\n",
        "                cur_json_index.append(0)\n",
        "\n",
        "            while len(cap_list) >= self.train_batch_size:\n",
        "                batch_cap = random.sample(cap_list, self.train_batch_size)\n",
        "                batch_index = [cap_list.index(cap) for cap in batch_cap]\n",
        "\n",
        "                batch_frames = [cap.read()[1] for cap in batch_cap]\n",
        "                batch_frames = img_to_tensor(batch_frames).to('cuda')\n",
        "\n",
        "                batch_cur_json_index = [cur_json_index[index] for index in batch_index]\n",
        "                batch_env_actions = [json_action_to_env_action(json_data_list[index][json_index])[0] for index, json_index in zip(batch_index, batch_cur_json_index)]\n",
        "                batch_target_actions_C = torch.tensor([act[0] for act in batch_env_actions]).to('cuda')\n",
        "                batch_target_actions_R = torch.tensor([act[1] for act in batch_env_actions], dtype=torch.float).to('cuda')\n",
        "\n",
        "                # Train C (ViT classification)\n",
        "                self.optimizerVIT.zero_grad()\n",
        "                batch_output_C = self.VIT(batch_frames)\n",
        "                loss_C = F.cross_entropy(batch_output_C, batch_target_actions_C)\n",
        "                if self.gradient_accumulation_steps > 1:\n",
        "                    loss_C = loss_C / self.gradient_accumulation_steps\n",
        "                loss_C.backward()\n",
        "\n",
        "                if (global_step + 1) % self.gradient_accumulation_steps == 0:\n",
        "                    losses.update(loss_C.item() * self.gradient_accumulation_steps)\n",
        "\n",
        "                    torch.nn.utils.clip_grad_norm_(self.VIT.parameters(), max_grad_norm)\n",
        "\n",
        "                    self.schedulerVIT.step()\n",
        "                    self.optimizerVIT.step()\n",
        "                    self.optimizerVIT.zero_grad()\n",
        "\n",
        "                    global_step += 1\n",
        "\n",
        "                # Train R (AlexNet regression)\n",
        "                self.optimizerR.zero_grad()\n",
        "                batch_output_R = self.policyR(batch_frames)\n",
        "                loss_R = self.lossRFunc(batch_output_R, batch_target_actions_R)\n",
        "                loss_R.backward()\n",
        "                self.optimizerR.step()\n",
        "\n",
        "                # Log and reset\n",
        "                batch_loss_C += loss_C.item()\n",
        "                batch_loss_R += loss_R.item()\n",
        "                batch_count += 1\n",
        "                if batch_count % 100 == 0:\n",
        "                    print(f'Batch {batch_count - 100} to {batch_count - 1}, LossC: {batch_loss_C}, LossR: {batch_loss_R}')\n",
        "                    batch_loss_C = 0\n",
        "                    batch_loss_R = 0\n",
        "\n",
        "                # Update indices and clean up\n",
        "                del_indices = []\n",
        "                for index in batch_index:\n",
        "                    cur_json_index[index] += 1\n",
        "                    if cur_json_index[index] >= len(json_data_list[index]):\n",
        "                        del_indices.append(index)\n",
        "\n",
        "                del_indices.sort(reverse=True)\n",
        "                for index in del_indices:\n",
        "                    cap_list[index].release()\n",
        "                    del cap_list[index]\n",
        "                    del json_data_list[index]\n",
        "                    del cur_json_index[index]\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "            losses.reset()\n",
        "\n",
        "            # Release remaining captures\n",
        "            for cap in cap_list:\n",
        "                cap.release()\n",
        "\n",
        "        self.save_model_weights()\n",
        "\n",
        "    def test(self):\n",
        "        files = glob.glob(\"/content/MineRLBasaltFindCave-v0/*.mp4\")\n",
        "        video_path = files[0]\n",
        "        json_path = video_path.replace(\".mp4\", \".jsonl\")\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "        frames.append(frame[::-1])\n",
        "        ret, frame = cap.read()\n",
        "        frames.append(frame[::-1])\n",
        "        frames_tensor = img_to_tensor(frames).to('cuda')\n",
        "        result = self.VIT(frames_tensor).detach()\n",
        "        print(result, type(result))\n",
        "\n",
        "    def predict(self, observe):\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = img_to_tensor([observe]).to('cuda')\n",
        "            resultC = self.VIT(obs_tensor).squeeze().argmax().cpu().numpy()\n",
        "            resultR = self.policyR(obs_tensor).squeeze().cpu().numpy()\n",
        "            env_action = agent_action_to_env(resultC, resultR)\n",
        "        return env_action\n",
        "\n",
        "    def save_model_weights(self, path=\"minerl_weights.pth\"):\n",
        "        torch.save({\n",
        "            'VIT_state_dict': self.VIT.state_dict(),\n",
        "            'optimizerVIT_state_dict': self.optimizerVIT.state_dict(),\n",
        "            'policyR_state_dict': self.policyR.state_dict(),\n",
        "            'optimizerR_state_dict': self.optimizerR.state_dict(),\n",
        "        }, path)\n",
        "\n",
        "    def load_model_weights(self, path=\"minerl_weights.pth\"):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.VIT.load_state_dict(checkpoint['VIT_state_dict'])\n",
        "        self.optimizerVIT.load_state_dict(checkpoint['optimizerVIT_state_dict'])\n",
        "        self.policyR.load_state_dict(checkpoint['policyR_state_dict'])\n",
        "        self.optimizerR.load_state_dict(checkpoint['optimizerR_state_dict'])\n",
        "\n",
        "\n",
        "TA = FindCaveAgent()\n",
        "TA.train()"
      ],
      "metadata": {
        "id": "GsdBpzdgLEBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "2dgy3nnVR9As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/minerl_weights.pth\"\n",
        "\n",
        "TA.load_model_weights(path)\n",
        "env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
        "\n",
        "disp = Display(visible=0, backend=\"xvfb\")\n",
        "disp.start();"
      ],
      "metadata": {
        "id": "BtUGtSq1S1AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def testing(agent, env, render=False):\n",
        "\n",
        "    obs = env.reset()\n",
        "    pov = obs[\"pov\"]\n",
        "\n",
        "    done = False\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        ac = agent.predict(pov)\n",
        "        obs, reward, done, info = env.step(ac)\n",
        "        pov = obs[\"pov\"]\n",
        "\n",
        "        cumulative_reward += reward\n",
        "\n",
        "        if render:\n",
        "            plt.imshow(pov)\n",
        "            plt.show()\n",
        "            plt.clf()  # Important to reduce the usage of RAM\n",
        "\n",
        "    print(f\"Total Cumulative Reward: {cumulative_reward}\")\n",
        "    return cumulative_reward\n",
        "\n",
        "\n",
        "render = False\n",
        "num_runs = 10\n",
        "cumulative_rewards = []\n",
        "\n",
        "for _ in range(num_runs):\n",
        "    cumulative_reward = testing(TA, env, render=render)\n",
        "    cumulative_rewards.append(cumulative_reward)\n",
        "\n",
        "average_cumulative_reward = sum(cumulative_rewards) / num_runs\n",
        "print(f\"Average Cumulative Reward over {num_runs} runs: {average_cumulative_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVBzDq72R_Kl",
        "outputId": "2d0aadbd-59ba-416a-b0b7-31b2e1b36296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Cumulative Reward: 0.0\n",
            "Total Cumulative Reward: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Others"
      ],
      "metadata": {
        "id": "rJRSJr3USFhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(args, model, output_dir, train_batch_size, gradient_accumulation_steps, loggername, learning_rate, weight_decay,\n",
        "                              \\seed, n_gpu, num_steps, local_rank, warmup_steps, max_grad_norm):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if local_rank in [-1, 0]:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        writer = SummaryWriter(log_dir=os.path.join(\"logs\", loggername))\n",
        "\n",
        "    train_batch_size = train_batch_size // gradient_accumulation_steps\n",
        "\n",
        "    # Prepare optimizer and scheduler\n",
        "\n",
        "    # Distributed training\n",
        "    # if args.local_rank != -1:\n",
        "    #     model = DDP(model, message_size=250000000, gradient_predivide_factor=get_world_size())\n",
        "\n",
        "    # Train!\n",
        "\n",
        "    model.zero_grad()\n",
        "    set_seed(seed, n_gpu)  # Added here for reproducibility (even between python 2 and 3)\n",
        "    losses = AverageMeter()\n",
        "    global_step = 0\n",
        "    while True:\n",
        "        model.train()\n",
        "        # epoch_iterator = tqdm(train_loader,\n",
        "        #             desc=\"Training (X / X Steps) (loss=X.X)\",\n",
        "        #             bar_format=\"{l_bar}{r_bar}\",\n",
        "        #             dynamic_ncols=True,\n",
        "        #             disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            x, y = batch\n",
        "            loss = model(x, y)\n",
        "\n",
        "            if gradient_accumulation_steps > 1:\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                losses.update(loss.item() * gradient_accumulation_steps)\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "                scheduler.step()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "                if local_rank in [-1, 0]:\n",
        "                    writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
        "                    writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
        "                # if global_step % args.eval_every == 0 and args.local_rank in [-1, 0]:\n",
        "                #     accuracy = valid(args, model, writer, test_loader, global_step)\n",
        "                #     if best_acc < accuracy:\n",
        "                #         save_model(args, model)\n",
        "\n",
        "                if global_step % t_total == 0:\n",
        "                    break\n",
        "        losses.reset()\n",
        "        if global_step % t_total == 0:\n",
        "            break\n",
        "\n",
        "    if local_rank in [-1, 0]:\n",
        "        writer.close()\n",
        "    logger.info(\"Best Accuracy: \\t%f\" % best_acc)\n",
        "    logger.info(\"End Training!\")\n"
      ],
      "metadata": {
        "id": "v0lhXRIPzka9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmalpPQeHy0X"
      },
      "outputs": [],
      "source": [
        "disp = Display(visible=0, backend=\"xvfb\")\n",
        "disp.start();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7BT_7Ss9HBl"
      },
      "outputs": [],
      "source": [
        "env.action_space.sample().keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6HkWKSIJgL1"
      },
      "outputs": [],
      "source": [
        "# Have a look at a few actions we might do:\n",
        "for _ in range(10):\n",
        "  print( env.action_space.sample() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3FGwWZ489mT"
      },
      "outputs": [],
      "source": [
        "# Now that Steve has been spawned, do some actions...\n",
        "t0=time.time()\n",
        "obs = env.reset()\n",
        "pov = obs[\"pov\"]\n",
        "print(f\"{(time.time()-t0):.2f}sec for env.reset\")\n",
        "\n",
        "done, iter = False, 0\n",
        "actionClist = [128, 128, 128, 130, 130, 130, 130, 130, 130, 128, 128, 0, 0, 0, 1]\n",
        "while not done:\n",
        "    ac = agent_action_to_env(actionClist[iter], [0, 0])\n",
        "    # ac = TA.predict(pov)  # Use this to test the performance of NN\n",
        "    # Spin around to see what is around us\n",
        "    # ac[\"camera\"] = [0, +30]  # (pitch, yaw) deltas in degrees : +30 => turn to right\n",
        "\n",
        "    t1=time.time()\n",
        "    obs, reward, done, info = env.step(ac)\n",
        "    #print(obs, reward, info)  # NB: Yikes : obs is only the image!\n",
        "    #  obs = Dict(pov:Box(low=0, high=255, shape=(360, 640, 3)))\n",
        "    #print(pov.shape) # (360, 640, 3)  Image spec agrees with docs!\n",
        "    print(f\"{(time.time()-t1):.2f}sec for env.step\")  # Approx 0.25sec per step\n",
        "\n",
        "    pov = obs[\"pov\"]\n",
        "\n",
        "    #env.render()  # This does an internal cv2.imshow that colab rejects\n",
        "    #cv2_imshow(pov[:, :, ::-1])\n",
        "    #cv2.waitKey(1)\n",
        "\n",
        "    plt.imshow(pov)\n",
        "    plt.show()\n",
        "    plt.clf()  # important to reduce the usage of RAM\n",
        "    iter +=1\n",
        "    if iter>22: done=True\n",
        "\n",
        "plt.close()\n",
        "\n",
        "f\"{(time.time()-t0):.2f}sec for whole spin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytLkHyBow1Ct"
      },
      "outputs": [],
      "source": [
        "# Set up a simple testing function\n",
        "def action_step(action):\n",
        "  ac = env.action_space.noop()\n",
        "  ac.update(action)\n",
        "  obs, reward, done, info = env.step(ac)\n",
        "  plt.imshow(obs[\"pov\"])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9hUkcZhuObe"
      },
      "outputs": [],
      "source": [
        "action_step({})\n",
        "action_step(dict(inventory=[1]))\n",
        "action_step(dict(camera=[0, +30]))\n",
        "action_step(dict(camera=[-10, -30]))\n",
        "action_step(dict(camera=[+10, 0]))\n",
        "action_step(dict(inventory=[1]))  # Put inventory away? = Yes, if it is showing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taCclVGyuSuZ"
      },
      "outputs": [],
      "source": [
        "#action_step({'inventory':[1]})  # Put inventory away? = NOT jump, sneak, use, hotbar.X, back\n",
        "action_step({})  # NOOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssp9BtYZ3u4g"
      },
      "outputs": [],
      "source": [
        "# Set up a simple calibration function\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def action_step_calibrate(x_off,y_off):\n",
        "  ac = env.action_space.noop()\n",
        "  ac.update(dict(camera=[y_off, x_off]))\n",
        "  obs, reward, done, info = env.step(ac)\n",
        "  im = obs[\"pov\"][100:250, 200:400,:]\n",
        "  cv2_imshow(cv2.cvtColor(im, cv2.COLOR_RGB2BGR))\n",
        "  ac = env.action_space.noop()\n",
        "  ac.update(dict(camera=[-y_off, -x_off]))  # Move back\n",
        "  obs, reward, done, info = env.step(ac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ung8m4tp3ueX"
      },
      "outputs": [],
      "source": [
        "action_step({})\n",
        "action_step(dict(inventory=[1]))\n",
        "\n",
        "action_step_calibrate(0, 0)\n",
        "for x_off in [+0.62, +1.61, +3.22, +5.81, +10.0]:\n",
        "  print(f\"x_off={x_off}\")\n",
        "  action_step_calibrate(x_off,0)\n",
        "  action_step_calibrate(-x_off,0)\n",
        "for y_off in [+0.62, +1.61, +3.22, +5.81, +10.0]:\n",
        "  print(f\"y_off={y_off}\")\n",
        "  action_step_calibrate(0, y_off)\n",
        "  action_step_calibrate(0, -y_off)\n",
        "\n",
        "action_step(dict(inventory=[1]))  # Put inventory away? = Yes, if it is showing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvxBJAcA73fv"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERSdDSW61fnw"
      },
      "outputs": [],
      "source": [
        "disp.stop();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO__56dWX3en"
      },
      "outputs": [],
      "source": [
        "# THE END! - We'll be using this set-up in the future!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}